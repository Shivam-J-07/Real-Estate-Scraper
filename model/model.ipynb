{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrrZoDDPQzPH"
      },
      "source": [
        "# Rental Listing Price Model\n",
        "\n",
        "Below are the steps taken to build our regression model which will be used to predict effective prices for prospective rental listings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfBHc_LpQzPL"
      },
      "source": [
        "## Preparing the Data\n",
        "\n",
        "First we need to clean and standardize the data scraped from the rental listing site in order to have the model train on it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_R2jkqugQzPM"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# Assuming your desired directory is one level up from the notebook's directory\n",
        "current_dir = os.getcwd()\n",
        "parent_dir = os.path.dirname(current_dir)\n",
        "\n",
        "sys.path.append(current_dir)\n",
        "sys.path.append(parent_dir)\n",
        "\n",
        "from data.data_cleaner import get_cleaned_data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bRf6e4FQzPN"
      },
      "source": [
        "### Data Cleaning\n",
        "`get_cleaned_data()` removes invalid and outlier data including blanks and data for single room listings. It also formats the building and unit amenities by making each column a dict that contains the relevant amenities as keys with a value of 1 if the listing has it, else 0.\n",
        "\n",
        "`flatten_data()` flattens the building and unit amenities to put individual amenities into their own columns, essentially flattening the building and unit amenities dicts into separate columns in each row."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eX7i73ytQzPN",
        "outputId": "a32f4069-2aec-4507-c2ef-95b5c0c2c8d7"
      },
      "outputs": [],
      "source": [
        "from data.data_cleaner import get_cleaned_df\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "df = get_cleaned_df()\n",
        "\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTP4IVSsQzPO",
        "outputId": "fa21eec1-1458-4ab9-92eb-677835ab030a"
      },
      "outputs": [],
      "source": [
        "print(\"Printing columns:\")\n",
        "print(df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBqux6O8QzPP",
        "outputId": "946b4200-fae2-4543-b471-b1d6b05b1d48"
      },
      "outputs": [],
      "source": [
        "print(\"Printing first 2 rows:\")\n",
        "print(df.head(2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tulbZCsQQzPP"
      },
      "source": [
        "### The `Building` and `UnitType` class\n",
        "\n",
        "For our purposes, we want to group the data by building type, unit type, and city as three major parameters. We created the `Building`, `UnitType`, `City` classes to group data together cleanly. This will become useful when dividing our data into a training and test set.\n",
        "\n",
        "The `Building` class encompasses the relationship between a building name and the different types of units in it.\n",
        "\n",
        "The `UnitType` class represents the different types of units where units are distinguished by number of bedrooms.\n",
        "\n",
        "The `City` class contains all the `Building` values associated with a specific city."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyt-tv6zQzPP"
      },
      "outputs": [],
      "source": [
        "from constants import TableHeaders\n",
        "from classes import Building, City"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khPJ4ziGQzPQ"
      },
      "outputs": [],
      "source": [
        "from classes import Building, City, convert_df_to_classes\n",
        "\n",
        "cities: list[City] = convert_df_to_classes(df)\n",
        "\n",
        "for city in cities:\n",
        "    for building in city.buildings[:5]:\n",
        "        print(building)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqQOFsDeQzPQ"
      },
      "source": [
        "Since we want to partition the data into a test and train set with an even 20% split based on the unit type, let's remove the entries that have less than 5 listings for that unit type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-ZYsetfQzPQ"
      },
      "outputs": [],
      "source": [
        "standardized_df = df.copy()\n",
        "print(df.head())\n",
        "city_groups = df.groupby(TableHeaders.CITY.value)\n",
        "for city_name, city_df in city_groups:\n",
        "    unit_groups = city_df.groupby(TableHeaders.BED.value)\n",
        "    for unit_type, unit_df in unit_groups:\n",
        "        # Filter out the unit listings that have less than 5 entries for that unit type\n",
        "        # since it won't have sufficient data to split between testing and training\n",
        "        if len(unit_df) < 5:\n",
        "            # print(city_name, unit_type, len(unit_df))\n",
        "            standardized_df = standardized_df.loc[\n",
        "                ~((standardized_df[TableHeaders.CITY.value] == city_name) &\n",
        "                (standardized_df[TableHeaders.BED.value] == unit_type))\n",
        "            ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zR7SIBKuQzPR"
      },
      "source": [
        "## Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Yc5TdB0QzPR"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "city_groups = standardized_df.groupby(TableHeaders.CITY.value)\n",
        "print(standardized_df.head())\n",
        "master_train_df = pd.DataFrame()\n",
        "master_test_df = pd.DataFrame()\n",
        "for city_name, city_df in city_groups:\n",
        "    train_df, test_df = train_test_split(city_df, test_size=0.2, random_state=42, stratify=city_df[TableHeaders.BED.value])\n",
        "    # Concatenate the individual city train and test sets with the master DataFrames\n",
        "    master_train_df = pd.concat([master_train_df, train_df], ignore_index=True)\n",
        "    master_test_df = pd.concat([master_test_df, test_df], ignore_index=True)\n",
        "\n",
        "master_train_1_bed = len(master_train_df.loc[master_train_df[TableHeaders.BED.value] == 1])\n",
        "master_test_1_bed = len(master_test_df.loc[master_test_df[TableHeaders.BED.value] == 1])\n",
        "master_train_2_bed = len(master_train_df.loc[master_train_df[TableHeaders.BED.value] == 2])\n",
        "master_test_2_bed = len(master_test_df.loc[master_test_df[TableHeaders.BED.value] == 2])\n",
        "\n",
        "print(f\"{(master_train_1_bed/master_train_2_bed):.2f}, {(master_test_1_bed/master_test_2_bed):.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9z_WSn-QzPR",
        "outputId": "1d5ea056-1a15-444d-801a-b7da476c54b0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "# Assuming 'target_column' is the name of your target variable\n",
        "dropped_columns = [\n",
        "    TableHeaders.PRICE.value,\n",
        "    TableHeaders.BUILDING.value,\n",
        "    TableHeaders.NEIGHBOURHOOD.value,\n",
        "    TableHeaders.CITY.value,\n",
        "    TableHeaders.LISTING.value,\n",
        "    TableHeaders.ADDRESS.value,\n",
        "    TableHeaders.DATE.value,\n",
        "    # TableHeaders.LAT.value,\n",
        "    # TableHeaders.LON.value,\n",
        "    # TableHeaders.PETS.value,\n",
        "    # TableHeaders.SQFT.value,\n",
        "    # TableHeaders.BED.value,\n",
        "    # TableHeaders.BATH.value,\n",
        "    # 'Balcony',\n",
        "    # 'In Unit Laundry',\n",
        "    # 'Air Conditioning',\n",
        "    # 'High Ceilings',\n",
        "    # 'Furnished',\n",
        "    # 'Hardwood Floor',\n",
        "    # 'Controlled Access',\n",
        "    # 'Fitness Center',\n",
        "    # 'Swimming Pool',\n",
        "    # 'Roof Deck',\n",
        "    # 'Storage',\n",
        "    # 'Residents Lounge',\n",
        "    # 'Outdoor Space',\n",
        "]\n",
        "\n",
        "# updated_df = master_train_df.loc[master_train_df[TableHeaders.CITY.value] == 'toronto']\n",
        "\n",
        "updated_train_df = master_train_df.drop(dropped_columns, axis=1)\n",
        "updated_test_df = master_test_df.drop(dropped_columns, axis=1)\n",
        "\n",
        "X_train = torch.tensor(updated_train_df.values).float()\n",
        "y_train = torch.tensor(master_train_df[TableHeaders.PRICE.value].values).float()\n",
        "\n",
        "X_test = torch.tensor(updated_test_df.values).float()\n",
        "y_test = torch.tensor(master_test_df[TableHeaders.PRICE.value].values).float()\n",
        "\n",
        "# y_train = y_train.view(y_train.shape[0],1)\n",
        "# y_test = y_test.view(y_test.shape[0],1)\n",
        "for index, column in enumerate(updated_train_df.columns):\n",
        "    print(column, X_train[0][index].item())\n",
        "\n",
        "print(\"Unit Price: \", y_train[0].item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0DWILz9QzPS"
      },
      "outputs": [],
      "source": [
        "from dataset import RentalDataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataset = RentalDataset(X_train, y_train)\n",
        "test_dataset = RentalDataset(X_test, y_test)\n",
        "\n",
        "batch_size = 32  # Choose a batch size that fits your model and training process\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y94bQ07XBCnI",
        "outputId": "873412bc-8788-4c4d-affe-90fefcfcc2c8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "NUM_EPOCHS = 50\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "class RegressionModelV2(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(RegressionModelV2, self).__init__()\n",
        "        # Increasing the complexity of the model\n",
        "        self.fc1 = nn.Linear(input_size, 64)\n",
        "        self.fc2 = nn.Linear(64, 64)\n",
        "        self.fc3 = nn.Linear(64, 32)\n",
        "        self.fc4 = nn.Linear(32, 16)\n",
        "        self.fc5 = nn.Linear(16, 1)  # Single output for regression\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = F.relu(self.fc4(x))\n",
        "        x = self.fc5(x)  # No activation function for the last layer in regression\n",
        "        return x\n",
        "\n",
        "input_size = X_train.shape[1]\n",
        "\n",
        "# Replace input_size with the actual size of your input features\n",
        "model = RegressionModelV2(input_size)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)  # Learning rate can be adjusted\n",
        "\n",
        "model.train()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  for input, targets in train_loader:\n",
        "    prediction = model(input)\n",
        "    loss = criterion(prediction, targets)\n",
        "\n",
        "    optimizer.zero_grad()  # Clear existing gradients\n",
        "    loss.backward()  # Backpropagation\n",
        "    optimizer.step()      # Update model parameters\n",
        "\n",
        "  # Optional: Print the loss every epoch\n",
        "  print(f'Epoch [{epoch+1}/{NUM_EPOCHS}], Loss: {loss.item():.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 559
        },
        "id": "vawaNzl0QzPS",
        "outputId": "6c775faa-1869-48f1-dbd5-afaf2cf38089"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from math import sqrt\n",
        "\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "with torch.no_grad():  # Gradient computation is not needed for evaluation\n",
        "  # Assuming you have tensors 'predictions' and 'actuals' for your test set\n",
        "  predictions = model(X_test)  # X_test is your input tensor for the test set\n",
        "  actuals = y_test  # y_test is the corresponding actual values tensor for the test set\n",
        "\n",
        "  # Detach predictions and actuals from the computation graph if they require gradients\n",
        "  predictions = predictions.detach()\n",
        "  actuals = actuals.detach()\n",
        "\n",
        "  # Convert to numpy arrays if needed\n",
        "  predictions_np = predictions.numpy()\n",
        "  actuals_np = actuals.numpy()\n",
        "\n",
        "  # Calculate MSE and RMSE\n",
        "  mse = mean_squared_error(actuals_np, predictions_np)\n",
        "  rmse = sqrt(mse)\n",
        "\n",
        "  # Calculate MAE\n",
        "  mae = torch.mean(torch.abs(predictions - actuals)).item()\n",
        "\n",
        "  # Calculate R-squared\n",
        "  r2 = r2_score(actuals_np, predictions_np)\n",
        "\n",
        "  print(f'MAE: {mae}')\n",
        "  print(f'MSE: {mse}')\n",
        "  print(f'RMSE: {rmse}')\n",
        "  print(f'R-squared: {r2}')\n",
        "  # Code to evaluate the model on the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming y_test is your actual values and predictions is your model's predictions\n",
        "plt.scatter(y_test, predictions)\n",
        "plt.xlabel('Actual Prices')\n",
        "plt.ylabel('Predicted Prices')\n",
        "plt.title('Predicted vs. Actual Prices')\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=3)  # Diagonal line\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "yNe5Agagf39I",
        "outputId": "46fb4ae6-3ad0-4456-e26f-8ccd8c33fd45"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "regression_model = LinearRegression()\n",
        "\n",
        "regression_model.fit(X_train, y_train)\n",
        "\n",
        "predictions = regression_model.predict(X_test)\n",
        "\n",
        "print(f\"{regression_model.score(X_test, y_test):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.scatter(y_test, predictions)\n",
        "plt.xlabel('Actual Prices')\n",
        "plt.ylabel('Predicted Prices')\n",
        "plt.title('Linear Regression - Predicted vs. Actual Prices')\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=3)  # Diagonal line\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNGoO_ZFXwji",
        "outputId": "05e6b6bd-e6b3-4ae4-a738-5b6dbcfc4d08"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Create the Random Forest classifier\n",
        "random_forest = RandomForestRegressor(n_estimators=500, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "random_forest.fit(X_train, y_train)\n",
        "\n",
        "# Predict using the test set\n",
        "predictions = random_forest.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "print(f\"{random_forest.score(X_test, y_test):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "00MRMfbkb2Rp",
        "outputId": "cde92f9b-8fe9-419f-c1bc-be711b181dbf"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming y_test is your actual values and predictions is your model's predictions\n",
        "plt.scatter(y_test, predictions)\n",
        "plt.xlabel('Actual Prices')\n",
        "plt.ylabel('Predicted Prices')\n",
        "plt.title('Random Forest - Predicted vs. Actual Prices')\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=3)  # Diagonal line\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "lDZgtZHXYxuG",
        "outputId": "2a452381-c634-4567-d9fd-5e4256707b81"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Assuming 'rf_model' is your trained Random Forest model and 'feature_names' is the list of feature names\n",
        "importances = random_forest.feature_importances_\n",
        "indices = np.argsort(importances)\n",
        "\n",
        "plt.title('Feature Importances')\n",
        "plt.barh(range(len(indices)), importances[indices], align='center')\n",
        "plt.yticks(range(len(indices)), [updated_train_df.columns[i] for i in indices])\n",
        "plt.xlabel('Relative Importance')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from joblib import dump\n",
        "\n",
        "# Save random forest model\n",
        "dump(random_forest, 'random_forest_model.joblib')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Get predictions from each tree for the single data point\n",
        "single_point_predictions = np.array([tree.predict(single_data_point.reshape(1, -1)) for tree in model.estimators_])\n",
        "\n",
        "# Calculate mean and standard deviation for the single prediction\n",
        "mean_prediction = np.mean(single_point_predictions)\n",
        "std_deviation = np.std(single_point_predictions)\n",
        "\n",
        "# Prediction interval\n",
        "lower_bound = mean_prediction - 2 * std_deviation\n",
        "upper_bound = mean_prediction + 2 * std_deviation\n",
        "\n",
        "# Output the prediction and interval\n",
        "print(f\"Prediction: {mean_prediction}, Interval: [{lower_bound}, {upper_bound}]\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6 (main, Nov  2 2023, 04:39:43) [Clang 14.0.3 (clang-1403.0.22.14.1)]"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "a773ca2156051aca7017cb15099c88c58fe83f1ae2d7d039bbbb5c66e96948e8"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
